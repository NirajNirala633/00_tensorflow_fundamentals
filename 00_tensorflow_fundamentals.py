# -*- coding: utf-8 -*-
"""00_tensorflow_fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-AKDcGZOBSXj6C8FZ1tB0jjJ14_WW9NC

# In this notebook we're going to cover some of the most fundamental concepts of tensors using tensorflow

More Specifically we're going to cover:
* Introduction to tensors
* Getting information from tensors
* Manipulating Tensors
* Tensors and NumPy
* Using @tf.function (a way to speed up your regular Python functions)
* Using GPUs with TensorFlow (or TPUs)
* Exercises to try to yourself

# Introduction to Tensors
"""

# Import Tensorflow
import tensorflow as tf
print(tf.__version__)

# Creating tensors with tf.constants()
scalar = tf.constant(7)
scalar

# Check the number of dimensions of a tensor (ndim stands for number of dimension)
scalar.ndim

# Create a vector
vector = tf.constant([10, 10])
vector

# Check the dimension of vector
vector.ndim

# Create a matrix (has more than one dimension)
matrix = tf.constant([[10, 7],
                     [7, 10]])
matrix

# Check dimension of matrix
matrix.ndim

# Create another matrix
another_matrix = tf.constant([[10., 7.],
                              [6., 9.],
                              [42., 24]], dtype=tf.float16) # Specify the data type with dtype parameter

another_matrix

# Check  the dimensions of another matrix
another_matrix.ndim

# Let's create a tensor
tensor = tf.constant([[[1, 2, 3],
                       [4, 5, 6]],
                      [[7, 8, 9],
                       [10, 11, 12]],
                      [[13, 14, 15],
                       [16, 17, 18]]])
tensor

# Check the dimension of tensor
tensor.ndim

"""What we've created so far
* Scalar a single number
* Vector: a number with direction(e.g. wind speed and direction)
* Matrix: a 2-dimensional array of numbers
* Tensor: an n-dimensionalarray of numbers(where n can be any number, a 0-dimensional tensor is a scalar, a 1-dimensional tensor is a vector)

# Creating tensors with `tf.Varible`
"""

tf.Variable

# Create a same tensor with tf.Variable() as above
changeable_tensor = tf.Variable([10, 7])
unchangeable_tensor = tf.constant([10, 7])

changeable_tensor, unchangeable_tensor

# Let's try to change one of the elements in our changeable tensor
changeable_tensor[0] = 7
changeable_tensor

# How about we try .assign()
changeable_tensor[0].assign(9)

# Let's try change our unchangeable tensor
unchangeable_tensor[0].assign(9)

"""# Creating Random Tensors

Random tensors are tensors of som arbitary size which contain random numbers.
"""

# Create two random (but the same) tensors
random_1 = tf.random.Generator.from_seed(42) # set seed for reproduceblity
random_1 = random_1.normal(shape=(3, 2))

random_2 = tf.random.Generator.from_seed(42)
random_2 = random_2.normal(shape=(3, 2))

# Are they equal
random_1, random_2, random_1 == random_2

"""# Shuffle the order of elements in tensor"""

# Shuffle a tensor (valueable when you want to shuffle your data so the inherent order doesn't effect learning)
not_shuffled = tf.constant([[6, 9],
                            [9, 6],
                            [42, 66]])
not_shuffled.ndim

not_shuffled

# Shuffle our non-shuffled tensor
tf.random.shuffle(not_shuffled)

# Shuffle our non-shuffled tensor
tf.random.set_seed(42)
tf.random.shuffle(not_shuffled, seed=42)

tf.random.set_seed(42) # global level random seed
tf.random.shuffle(not_shuffled, seed=42) # operationn level random seed

"""### Other ways to make tensors"""

# Create a tensor of all ones
tf.ones([6, 9])

# Create a tensor of all zeros
tf.zeros([9, 6])

tf.zeros(shape=(3, 4))

"""# Turn Numpy arrays into tensors

The main difference betwwen NumPy arrays and Tensorflow tensors is that tensors can be run on a GPU (much faster for numerical computing).
"""

# You can also turn NumPy arrays into tensors
import numpy as np
numpy_A = np.arange(1, 25, dtype=np.int32) # create a numpy range of 1 to 25
numpy_A

# X = tf.constant(some_matrix) # capital for matrix or tensor
# y = tf.constant(vector) #non-capital for vector

A = tf.constant(numpy_A, shape=(2, 3, 4))
B = tf.constant(numpy_A)
A, B

A = tf.constant(numpy_A, shape=(3, 8))
B = tf.constant(numpy_A)
A, B

"""# Getting more infomation from tensors

when dealing with tensors you probably want to aware of the following atrributes:
* Shape
* Rank
* Axis or Dimension
* Size
"""

# Create a rank 4 tensor (4 dimensions)
rank_4_tensor = tf.zeros(shape=[2, 3, 4, 5])
rank_4_tensor

rank_4_tensor[0]

rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)

# Get various attributes of our tensors
print("Datatype of every element:", rank_4_tensor.dtype) 
print("Number of Dimensions: (rank)", rank_4_tensor.ndim)
print("Shape of a tensor:", rank_4_tensor.shape)
print("Elements along the 0 axis:", rank_4_tensor.shape[0])
print("Elements along the last axis:", rank_4_tensor.shape[-1])
print("Total numbers of elements in our tensor:", tf.size(rank_4_tensor))
print("Total numbers of elements in our tensor:", tf.size(rank_4_tensor).numpy())

"""# Indexing Tensors

Tensors can be indexed just like Python Lists.
"""

some_list = [1, 2, 3, 4]
some_list[:2]

# Get the first 2 elements of each dimension 
rank_4_tensor[:2, :2, :2, :2]

some_list[:1]

# Get the first  element from each  dimension from each index except the final one 
rank_4_tensor[:1, :1, :1]

rank_4_tensor[:1, :1, :, :1]

rank_4_tensor[:1, :, :1, :1]

rank_4_tensor[:, :1, :1, :1]

# create a rank 2 tensor (2-dimensions)
rank_2_tensor = tf.constant([[10, 7],
                             [3, 4]])
rank_2_tensor, rank_2_tensor.shape, rank_2_tensor.ndim

# Get the last item of each row of our rank 2 tensor
rank_2_tensor[:, -1]

# Add in extra dimension to our rank_2_tensor
rank_3_tensor = rank_2_tensor[..., tf.newaxis]
rank_3_tensor

# Altenative to tf.newaxis
tf.expand_dims(rank_2_tensor, axis=-1) #"-1" means expand the final axis

# Expand 0-axis
tf.expand_dims(rank_2_tensor, axis=0) #expand the 0-axis

tf.expand_dims(rank_2_tensor, axis=1)

"""### Manipulating Tensors (Tensor Operations)
**Basic Operation**
`+`, `-`, `*`, `/`
"""

# You can add values to tensor using the addition operator
tensor = tf.constant([[10, 7],
                      [3, 4]])
tensor + 10

# Original tensor is unchanged
tensor

# Multiplication
tensor * 10

# We can use tensorflow builtin-function to multiply
tf.multiply(tensor, 10)

# Substraction
tensor - 9

# Division
tensor / 6

"""# Matrix Multiplication
In machine learning, matrix multiplication is one of the most common tensor operations.
"""

# Matrix multiplication in tensorflow
print(tensor)
tf.matmul(tensor, tensor)

# Matrix multiplication with Python operator "@"
tensor @ tensor

# Create a tensor of (3, 2)
X = tf.constant([[1, 2],
                 [3, 4],
                 [5, 6]])
# Create another (3, 2) tensor
Y = tf.constant([[7, 8],
                 [9, 10],
                 [11, 12]])
X, Y

"""There are two rules our tensors (or matrices) need to fulfil if we're going to matrix multiply them:

1. The inner dimensions must match
2. The resulting matrix has the shape of the outer dimension.
"""

# Try to matrix multiply tensors of same shape
tf.matmul(X, Y)

# Let's change of Y
tf.reshape(Y, shape=(2, 3))

X.shape, tf.reshape(Y, shape=(2, 3)).shape

# Try to multiply X by reshaped Y
X @ tf.reshape(Y, shape=(2, 3))

tf.matmul(X, tf.reshape(Y, shape=(2, 3)))

# Try changing the shape of X instead of Y
tf.matmul(tf.reshape(X, shape=(2, 3)), Y)

tf.reshape(X, shape=(2, 3)).shape, Y.shape

# Can do the same with transpose
X, tf.transpose(X), tf.reshape(X, shape=(2, 3))

# Try matrix multiplication with transpose rather than reshape
tf.matmul(tf.transpose(X), Y)

"""**the dot product**

 Matrix multiplication is also refferrred to as the dot product:

You can perform matrix multiplication using:
* `tf.matmul()`
* `tf.tensordot()`
* `@`
"""

# Perform the dot product on X and Y (requires X or Y to be transposed)
tf.tensordot(tf.transpose(X), Y, axes=1)

# Perform matrix multiplication between X and Y (transposed)
tf.tensordot(X, tf.transpose(Y), axes=1)

# Perform matrix multiplication between X and Y (reshaped)
tf.tensordot(X, tf.reshape(Y, shape=(2, 3)), axes=1)

# Check the values of Y, reshape Y, and transposed Y
print("Normal Y:")
print(Y, "/n")

print("Y reshaped to (2, 3):")
print(tf.reshape(Y, shape=(2, 3)))

print("Y transposed")
print(tf.transpose(Y))

"""Generally when performing matrix multiplication on two tensors and one of axes don't line up, you will transpose (rather than reshape) one of the tensors to satisfy the matrix multiplication rules.

# Change the datatype of a tensor
"""

tf.__version__

# Create a new tensor with default datatype (float32)
B = tf.constant([1.7, 7.4])
B, B.dtype

C = tf.constant([6, 9])
C.dtype

# Change the float32 to float16 (reduce precision)
D = tf.cast(B, dtype=tf.float16)
D, D.dtype

# Change from int32 to float32
E = tf.cast(C, dtype=tf.float32)
E, E.dtype

"""### Aggregating Tensors

Aggregating tensors = condensing them from multiple values down to a smaller amount of values.
"""

F = tf.constant([-7, -10])
F

# Get the absolute values
tf.abs(F)

"""Let's go through folllowing forms of aggregation:

* Get the minimum
* Get the maximum
* Get the mean of tensor
* Get the sum of tensor
"""

# Create a random tensor with values between 0 and 100 of size 50
G = tf.constant(np.random.randint(0, 100, size=50))
G

tf.size(G), G.shape, G.ndim

# Find the minimum
tf.reduce_min(G)

# Find the maximum
tf.reduce_max(G)

# Find the mean
tf.reduce_mean(G)

# Find the sum
tf.reduce_sum(G)

tfp.stats.variance(G)

import tensorflow_probability as tfp

tfp.stats.variance(G)

# Find standard deviation
tf.math.reduce_std(tf.cast(G, dtype=tf.float32))

"""### Find the positional maximum and minimum"""

# Create a new tensor for finding positional minimum and maximum
tf.random.set_seed(42)
H = tf.random.uniform(shape=[50])
H

# Find the positional maximum
tf.argmax(H)

# Index our largest value position
H[tf.argmax(H)]

# Find the max value of H
tf.reduce_max(H)

# Check for equality
H[tf.argmax(H)] == tf.reduce_max(H)

# Find the positional minimum
tf.argmin(H)

# Find the minimum using the positional minimum index
H[tf.argmin(H)]

"""### Squeezing a tensor (removing all single dimensions)"""

tf.random.set_seed(42)
I = tf.constant(tf.random.uniform(shape=[50]), shape=(1, 1, 1, 1, 50))
I

I.shape

I_squeezed = tf.squeeze(I)
I_squeezed, I_squeezed.shape

"""### One-hot encoding tensors"""

# Create a list of indicies
some_list = [0, 1, 2, 3] # could be red, green, blue, purple

# One hot encode our list of indicies
tf.one_hot(some_list, depth=4)

# Specify custom values for one hot encoding
tf.one_hot(some_list, depth=4, on_value="I love deep learning", off_value="I also like to design")

"""### Squaring, Log, Square Root"""

J = tf.range(1, 10)
J

# Square It
tf.square(J)

tf.sqrt(J)

# Find the squareroot
tf.sqrt(tf.cast(J, dtype=tf.float32))

# Find the log
tf.math.log(tf.cast(J, dtype=tf.float32))

"""### Tensors and NumPy

TensorFlow interacts beautifully with NumPy arrays.
"""

# Create a tensor directly from a NumPy array
K = tf.constant(np.array([3., 7., 10.]))
K

# Convert our tensor back to a NumPy array
np.array(K), type(np.array(K))

# Convert tensor K to NumPy array
K.numpy(), type(K.numpy())

# The default types of each are slightly different
numpy_K = tf.constant(np.array([3., 7., 10]))
tensor_K = tf.constant([3., 7., 10])
# Check the datatypes of each
numpy_K.dtype, tensor_K.dtype

# Finding access to GPUs
import tensorflow as tf
tf.config.list_physical_devices("GPU")

!nvidia-smi

